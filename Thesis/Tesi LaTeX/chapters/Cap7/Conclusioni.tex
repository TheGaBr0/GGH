\chapter{Conclusioni}

In questa tesi sono stati approfonditi argomenti chiave per la crittografia basata sui 
reticoli, con relative implementazioni pratiche per dimostrarne le effettive proprietà. 
Nello specifico i due soggetti chiave sono stati i crittosistemi GGH e GGH-HNF, quest'ultimo
una versione migliorata del primo. Gli studi eseguiti su di essi non miravano alla sola 
revisitazione degli stessi in un contesto molto più moderno rispetto a quello in cui sono 
nati, ma anche al tentativo di proporne miglioramenti ulteriori. I risultati ottenuti hanno 
dimostrato come l'avanzamento tecnologico abbia incrementato drasticamente le performance
di questi due crittosistemi, sia a livello hardware che software. È infatti notevole
osservare come un computer di fascia media odierno, se equipaggiato con software 
ottimizzato, sia in grado di superare nettamente le prestazioni dei computer di fascia 
alta di circa vent'anni fa. E' stato provato inoltre come un linguaggio di programmazione
considerato "lento" come Python sia in grado invece, attraverso opportune integrazioni 
con librerie di basso livello, a fornire alte prestazioni intrinseche ai suoi tanti vantaggi,
tra cui: leggibilità, flessibilità e possibilità di essere eseguito su piattaforme diverse 
senza dover installare ulteriori dipendenze. Un'ulteriore scoperta eseguita è legata 
alla versione ibrida tra GGH e GGH-HNF, che utilizza la generazione della chiave privata
di GGH integrata nella restante struttura di GGH-HNF. I dati di questa versione infatti 
spiccano tra le altre, dimostrando di essere migliore sotto tutti i punti di vista. 
Se però le prestazioni dei crittosistemi hanno giovato particolarmente dalle nuove tecnologie
usate, stessa cosa potrebbe essere infatti detta per gli attacchi su di essi. Tale supposizione risulta
parzialmente vera: gli attacchi hanno impiegato sensibilmente meno tempo per essere svolti, ma 
sia a causa dell'esponenzialità di BKZ che dei limiti teorici dei reticoli, essi non sono 
andati oltre alle dimensioni già precedentemente raggiunte. Nello specifico per GGH i dati 
sono rimasti coerenti con le scoperte di Nguyen in \cite{Nguyen99}, con una discrepanza 
in dimensione 350 dovuta alle differenze nelle scelte teorico-implementative usate. Per 
quanto riguarda GGH-HNF invece, gli studi precedenti si sono fermati in pratica a dimensione
280, calcolando attraverso proiezioni statistiche che una dimensione di 800 fosse necessaria 
per una sicurezza adeguata. I risultati sperimentali di questa tesi si sono fermati alla 
dimensione 400 con un tempo richiesto di circa 16 ore, è però verosimile ammettere che, 
un uso di hardware migliore e di un algoritmo di riduzione costruito ad hoc, potrebbero permettere
attacchi su dimensioni maggiori di essere raggiunti in un tempo accettabile. Le stesse 
considerazioni non possono però essere stanziate anche per la versione ibrida di GGH-HNF, la 
quale ha dimostrato una resistenza superiore a GGH originale, impiegandoci il doppio del 
tempo richiesto per rompere quest'ultimo in dimensione 300 e non andando oltre. E' molto 
importante questo dato in quanto questa versione non è
soggetta alla vulnerabilità scoperta da Nguyen, deve essere infatti attaccata usando la stessa 
metodologia usata per GGH-HNF. Ulteriori studi sulla sua sicurezza e le sue proprietà 
teoriche sono però necessari prima di affermare che essa possa risultare come un punto 
di svolta nel contesto di GGH.\\ Per concludere, una domanda che resta senza risposta è la seguente: 
GGH e le sue varianti potrebbero essere usate in ambito crittografico pratico? Sfortunatamente
considerando una dimensione di 800, ritenuta un limite pratico dove la sicurezza è provata, 
i dati dimostrano come 
alcune proprietà dei crittosistemi restino poco competitive rispetto ad altri schemi odierni 
come RSA. Il processo di decifratura richiede un minimo di 10 minuti, 
utilizzando un metodo che non garantisce il pieno successo nel recupero della soluzione, 
senza considerare la versione ibrida del crittosistema 
che non risulta essere impattata da questo fattore. Inoltre, ben più grave, sono le dimensioni delle
chiavi che non sono cambiate dai dati di circa vent'anni fa, neanche utilizzando algoritmi 
di compressione. Bisogna considerare però che non tutte le strategie di ottimizzazione 
disponibili sono state usate e che forse è possibile ridurre ulteriormente le 
dimensioni delle stesse. Anche se ulteriori approcci dovessero funzionare, sarebbe poco 
verosimile raggiungere le caratteristiche di RSA, dove le chiavi arrivano a qualche centinaio di bytes. 